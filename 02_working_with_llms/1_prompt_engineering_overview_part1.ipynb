{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TW7yEtaWCUQL"
   },
   "source": [
    "# Overview of Prompt Engineering Techniques & Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8TD6U9JCUQO"
   },
   "source": [
    "## Part 1: Prompt Engineering Best Practices\n",
    "\n",
    "In this section, we provide an overview of the top tips and best practices for prompting LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byL1RG2UCUQO"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMKUTILJCUQP"
   },
   "source": [
    "We first load the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from src.utils import setup_credentials, get_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F09KkfieCUQQ"
   },
   "outputs": [],
   "source": [
    "setup_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWBwLW6kCUQR"
   },
   "source": [
    "### Be Specific and Clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfhCFzCOCUQR"
   },
   "source": [
    "Write instructions as clear and specific as possible to get the desired LLM behaviors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vUWyqyT9CUQR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, couldn't find a movie to recommend today. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
    "\n",
    "system_message = \"\"\"\n",
    "Your task is to recommend movies to a customer.\n",
    "\n",
    "You are responsible to recommend a movie from the top global trending movies from {global_trending_movies}.\n",
    "\n",
    "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
    "\n",
    "If you don't have a movie to recommend or don't know the user interests, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
    "\"\"\"\n",
    "\n",
    "user_request = \"\"\"\n",
    "Please recommend a movie based on my interests.\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message.format(global_trending_movies=global_trending_movies)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_request\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6hb7537CUQS"
   },
   "source": [
    "The more specific the desired the behavior you want from the model, the more specific the instructions and logic should be. Below is an example where the customer provides information about interests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d1sfCPwKCUQT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should watch **Spider-Man: No Way Home**. It's a superhero movie with a lot of action and humor. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
    "\n",
    "system_message = \"\"\"\n",
    "Your task is to recommends movies to a customer.\n",
    "\n",
    "You are responsible to recommend a movie from the top global trending movies from {global_trending_movies}.\n",
    "\n",
    "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
    "\n",
    "If you don't have a movie to recommend or don't know the user interests, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
    "\"\"\"\n",
    "\n",
    "user_request = \"\"\"\n",
    "I love super-hero movies. Please recommend a movie based on my interests.\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message.format(global_trending_movies=global_trending_movies)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_request\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IZtFaB9CUQT"
   },
   "source": [
    "### Add Delimiters\n",
    "\n",
    "Adding delimiters help to better structure instructions and the overall prompt components. This is beneficial to get more reliable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Gy8_gFBnCUQU"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python```python\n",
       "strings2 = []\n",
       "strings2.append(\"one\")\n",
       "strings2.append(\"two\")\n",
       "strings2.append(\"THREE\")\n",
       "strings2.append(\"4\")\n",
       "``` \n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Convert the following code block in the #### <code> #### section to Python:\n",
    "\n",
    "####\n",
    "strings2.push(\"one\")\n",
    "strings2.push(\"two\")\n",
    "strings2.push(\"THREE\")\n",
    "strings2.push(\"4\")\n",
    "####\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "IPython.display.Markdown(\"```python\" + get_completion(message) + \"\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leL09Rl1CUQU"
   },
   "source": [
    "### Specify Output Format\n",
    "\n",
    "If the format of prompt responses are important, then this should be explicitly stated in the prompt to get desired results. In the example, we would like to export the results as a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9FBIuxnKCUQU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"product_name\": \"Nike Air Max 270 React\",\n",
      "  \"product_bran\": \"Nike\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Your task is: given a product description, return the requested information in the section delimited by ### ###. Format the output as a JSON object.\n",
    "\n",
    "Product Description: Introducing the Nike Air Max 270 React: a comfortable and stylish sneaker that combines two of Nike's best technologies. With a sleek black design and a unique bubble sole, these shoes are perfect for everyday wear.\n",
    "\n",
    "###\n",
    "product_name: the name of the product\n",
    "product_bran: the name of the brand (if any)\n",
    "###\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "print(get_completion(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8XnZQ3kCUQU"
   },
   "source": [
    "### Think Step by Step\n",
    "\n",
    "To elicit reasoning in LLMs, you can prompt the model to think step-by-step. Prompting the model in this way allows it to provide the details steps before providing a final response that solves the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uKQz73H8CUQV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the solution broken down into steps:\n",
      "\n",
      "1. **Identify the odd numbers:** 15, 5, 13, 7, 1 \n",
      "2. **Add the odd numbers:** 15 + 5 + 13 + 7 + 1 = 41\n",
      "3. **Indicate whether the result is odd or even:** 41 is an **odd** number. \n",
      "\n",
      "**Therefore, the odd numbers in the group add up to an odd number, not an even number.** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
    "\n",
    "Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response= get_completion(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYyOpLGuCUQV"
   },
   "source": [
    "### Role Playing\n",
    "\n",
    "The example below shows how to apply role playing using a chat model like GPT-3.5 Turbo. Notice the use of system message, user message, and assistant message in the example. You can combine different messages to mimic or jump start the behavior you want or expect from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FrXEZb-MCUQV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black holes are formed through the gravitational collapse of massive stars. Here's a breakdown of the process:\n",
      "\n",
      "**1. Stellar Evolution:**\n",
      "\n",
      "* Stars are born from giant clouds of gas and dust.\n",
      "* During their lifetime, stars fuse hydrogen into helium in their core, generating energy and outward pressure that balances the inward pull of gravity.\n",
      "* As a star ages, it exhausts its hydrogen fuel and begins fusing heavier elements.\n",
      "\n",
      "**2. Core Collapse:**\n",
      "\n",
      "* When a star runs out of fuel, its core can no longer support itself against gravity.\n",
      "* The core collapses rapidly, squeezing the matter into a smaller and smaller space.\n",
      "* This collapse releases immense energy, causing a supernova explosion.\n",
      "\n",
      "**3. Black Hole Formation:**\n",
      "\n",
      "* If the star's core is massive enough (greater than about 3 times the mass of the Sun), the collapse continues unabated.\n",
      "* The core becomes infinitely dense, creating a singularity – a point of infinite density and zero volume.\n",
      "* The intense gravity of the singularity pulls in all matter and light within a certain radius, forming the event horizon – the boundary beyond which nothing can escape.\n",
      "\n",
      "**Types of Black Holes:**\n",
      "\n",
      "* **Stellar-mass black holes:** Formed from the collapse of massive stars.\n",
      "* **Supermassive black holes:** Found at the centers of galaxies, with masses millions or billions of times that of the Sun.\n",
      "* **Intermediate-mass black holes:**  A hypothetical type of black hole with masses between stellar-mass and supermassive black holes.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "* **Singularity:** A point of infinite density and zero volume at the center of a black hole.\n",
      "* **Event horizon:** The boundary around a black hole beyond which nothing, not even light, can escape.\n",
      "* **Gravitational lensing:** The bending of light around massive objects, including black holes, which can create distorted images of objects behind them.\n",
      "\n",
      "**Observations and Research:**\n",
      "\n",
      "* Black holes are observed indirectly through their gravitational effects on surrounding matter.\n",
      "* Astronomers use telescopes and other instruments to detect the radiation emitted by matter falling into black holes.\n",
      "* The Event Horizon Telescope (EHT) has captured the first image of a black hole's shadow.\n",
      "\n",
      "**Further Research:**\n",
      "\n",
      "* The nature of singularities and the information paradox.\n",
      "* The role of black holes in galaxy formation and evolution.\n",
      "* The possibility of wormholes and other exotic phenomena related to black holes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"\n",
    "The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
    "\"\"\"\n",
    "\n",
    "user_message_1 = \"\"\"\n",
    "Hello, who are you?\n",
    "\"\"\"\n",
    "\n",
    "ai_message_1 = \"\"\"\n",
    "Greeting! I am an AI research assistant. How can I help you today?\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Human: Can you tell me about the creation of blackholes?\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message_1\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": ai_message_1\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
